{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.3 Gradient Boosting Guided Example and Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting guided example\n",
    "\n",
    "Having walked through gradient boost by hand, now let's try it with SKlearn.  We'll still use the European Social Survey Data, but now with a categorical outcome: Whether or not someone lives with a partner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv((\n",
    "    \"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "    \"master/ESS_practice_data/ESSdata_Thinkful.csv\")).dropna()\n",
    "\n",
    "# Definine outcome and predictors.\n",
    "# Set our outcome to 0 and 1.\n",
    "y = df['partner'] - 1\n",
    "X = df.loc[:, ~df.columns.isin(['partner', 'cntry', 'idno'])]\n",
    "\n",
    "# Make the categorical variable 'country' into dummies.\n",
    "X = pd.concat([X, pd.get_dummies(df['cntry'])], axis=1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're now working with a binary outcome, we've switched to a classifier.  Now our loss function can't be the residuals.  Our options are \"deviance\", or \"exponential\".  Deviance is used for logistic regression, and we'll try that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04609929078014184\n",
      "Percent Type II errors: 0.17757774140752863\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.05276073619631902\n",
      "Percent Type II errors: 0.18773006134969325\n"
     ]
    }
   ],
   "source": [
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike decision trees, gradient boost solutions are not terribly easy to interpret on the surface.  But they aren't quite a black box.  We can get a measure of how important various features are by counting how many times a feature is used over the course of many decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANgAAAEWCAYAAAAEtVmdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAH41JREFUeJztnXm8FcWZ978/AdkFEaJIUJQQDSKDiqgzqBiXUdSor/pighOIjsibOLiEOL6TSSQSd2NM1EjQGHGPe4waxVEY4y7I5oYrjFHUgAFBcAGf+aPqaHM4595z7z3dp8/1+X4+53O7q6qrnu57nlPV1b9+SmaG4zjpsFGtDXCc1ow7mOOkiDuY46SIO5jjpIg7mOOkiDuY46SIO1gGSNpK0ipJbSooO0LSXxvIv0bSz6troZMW7mBFSHpA0lkl0g+T9I6ktk2t08z+x8y6mNm66ljZPCSZpK/V0oYCkhZJ2q/WdqSNO9iGXAP8iyQVpf8LcIOZrW1KZc1xyNbMl+16uINtyF1AD2DPQoKkTYFDgGvj/sGS5kj6QNKbkiYlyvaLPcXxkv4HeDiR1jaW+Z6kFyWtlPS6pBOLjZD0H5KWxl/60eWMlXSIpLmSlkt6XNLgSk5S0iRJt0q6PtqxQNLXJf1/Se/F8zogUX6mpHMlPS1phaQ/SuqRyP+WpOejHTMlfSORt0jSv0uaD3wo6SZgK+BPceh8eix3axwlrJD0iKQdEnVcI+lySfdGe5+S1D+Rv4OkByW9L+ldSf8R0zeSdIak1yQtk3RL0u7UMTP/FH2AK4GrEvsnAnMT+yOAHQk/UIOBd4HDY14/wAjO2BnomEhrG8scDPQHBOwNrAZ2TtS9FrgYaB/zPwS2i/nXAD+P2zsD7wG7AW2AMcAioH2Z8zLga3F7EvAR8M9A22jvG8CPgXbACcAbiWNnAm8Bg+J53Q5cH/O+Hm3cPx57OvAqsHHMXwTMBfoCHRNp+xXZdxzQNZ73JUXX/BrgfWBYtPcG4OaY1xVYAvwQ6BD3d4t5pwBPAl+N9f4WuCmz71Ktv8x5/ADDgRWJL8NjwKkNlL8E+GWRg22byF/PwUocfxdwctwuOFjnRP4twE8SX7SCg10BTC6qayGwd5l2ih3swUTeocAqoI198aU1oHvcnwmclyg/EPiE4Ng/AW5J5G0UnXFE3F8EHFdkywYOVpTfPbbfLXHeyR+9kcBLcfvbwJwy9bwI7JvY7w18Wu5/Ue2PDxFLYGaPAn8DDpO0LbArcGMhX9JukmZI+pukFcB4oGdRNW+Wq1/SQZKejMOZ5YQvS/L4v5vZh4n9xcCWJaraGvhhHJYtj3X1LVO2FO8mttcAS+2LiZg18W+XRJnkOS0m9FY9Y3uLCxlm9lks26fMsRsgqY2k8+JQ7gOCA8L61+WdxPbqhG19gdfKVL01cGfi+rwIrAM2b8ieauEOVp5rge8SJjemm1nyy3gjcDfQ18y6AVMIw70kJV9TkNSeMLy6CNjczLoD9xUdv6mkzon9rYC3S1T3JnC2mXVPfDqZ2U0Vn2XT6Ftk06fA0mjb1oWMOEHUl9CLFSi+HsX73wEOA/YDuhF6fdjwupbiTcKQu1zeQUXXqIOZvVWmfFVxByvPtYR/9gnAtKK8rsD7ZvaRpGGEL0elbEy4F/gbsFbSQcABJcr9TNLGkvYkTLDcWqLMlcD42KNKUuc4AdO1CfY0hWMlDZTUCTgLuC32eLcAB0vaV1I7wr3Qx8DjDdT1LrBtYr9rPGYZ0Ak4pwl23QNsIekUSe0ldZW0W8ybApwtaWsASb0kHdaEuluEO1gZzGwR4QvSmdBbJfk+cJaklcBPCV+wSutdCUyIx/yd4JzF9b8T894m3MyPN7OXStQ1i/ADcFks/yowtlJbmsF1hHuhdwiTCROiHQuBY4FLCT3aocChZvZJA3WdC/xnHLpNJPygLSb0ei8QJiYqIl7T/WO77wCvAPvE7F8Rru/0+P96kjAplAmKN36O0yCSZhJmDa+qtS31hPdgjpMi7mCOkyI+RHScFPEezHFSpNUKL3v27Gn9+vWrtRlOK2X27NlLzaxXY+VarYP169ePWbNm1doMp5UiaXHjpXyI6Dip4g7mOCniDuY4KeIO5jgp4g7mOCniDuY4KeIO5jgp4g7mOCnSah80L3hrBf3OuLfWZjh1zKLzDm5xHd6DOU6KuIM5Toq4gzlOiqTqYJLukjQ7RnwdF9OOl/RyjP56paTLYnovSbdLeiZ+/immD4sRa+fEv9ulabPjVJO0JzmOM7P3JXUEnpF0LyFI5c7ASuBhYF4s+ytC8M5HJW0FPAB8A3gJ2MvM1iosFnAOcGSpxqITjwNos0mjbxI4Tuqk7WATJB0Rt/sSYgz+t5m9DyEWOSHsMoQQaQP1xZoLm8TwY92AaZIGEGLptSvXmJlNBaYCtO89wF/VdmpOag4maQTBafYws9UxKtFCQq9Uio1i2TXJREmXAjPM7AhJ/QghnB2nLkjzHqwbIQT0aknbA7sTAkruLWlThZVGkkO96cBJhR1JQxL1FKKwjk3RXsepOmk62P1A27hkzWRCwMe3CPdQTwH/RQgwuSKWnwAMlTRf0guEeO8AFwDnSnqMsNCA49QNmUeVktTFzFbFHuxO4Gozu7Pa7QwdOtQ8ZICTFpJmm9nQxsrV4jnYJElzgecI61HdVQMbHCcTMtcimtnErNt0nFrhYl+nUaohev2y4lIpx0mRqjiYwiLfz1WjLsdpTXgP5jgpUk0HaxPFu89Lmi6po6QTonB3XhTydgKQdI2kKZL+EoW/h8T0sZL+KOl+SQslnRnTJ0s6udCQpLMlTaii7Y6TCtV0sAHA5Wa2A7CcoNK4w8x2NbN/ICw+fXyifD9gb+BgYIqkDjF9GDAaGAIcLWko8DtgDICkjYBjCCs/roekcZJmSZq1bvWK4mzHyZxqOtgbZjY3bs8mONCg2EstIDjNDonyt5jZZ2b2CvA6sH1Mf9DMlkVN4h3A8Lic6zJJOxHWM55jZsuKDTCzqWY21MyGtunUrYqn5jjNo5rT9B8nttcBHQnr+R5uZvMkjQVGJMqUW3W+XPpVBC3iFsDVLbbWcTIg7UmOrsCSuPL86KK8oyVtJKk/YbX5hTF9f0k94jtkhwOPxfQ7gQOBXQnvijlO7kn7QfNPCMLexcACgsMVWAj8N7A5MN7MPorvgj1KWM3+a8CNZjYLwMw+kTQDWG5m61K223GqQlUcLN4jDUrsX5TIvqLMYY+Z2akl0t8zs5OKE+Pkxu7A0ZXYtGOfbsxyBYJTY+riOZikgcCrwENxUsRx6oJWuwh6+94DrPeYS2ptRllc31ff5Pl1Fcf50pCpg0maJGli3N5e0twYjq1/A8fcJ6l7dlY6TvWoZQ92OPBHM9vJzF4rV8jMRprZ8mSaAt77OrmnRV/SqKJ/SdK0GEvjNkmdJC2SdL6kp+Pna0XHjQROAf41Tr2XDFIa0xdJ6hnbelHSb4BnCWHgHCfXVKMX2A6YamaDgQ+A78f0D8xsGHAZsN5sg5ndB0whBBrdJyYfZ2a7AEMJ8RQ3K9PWtbHXW1yc6VpEJ29Uw8HeNLOC2uJ6YHjcvinxd48K6pkgaR4h+lRfgni4mMVm9mS5ClyL6OSNajxorkRT2OCzgDJBSjuUKPphM210nJpQjR5sK0mFHurbBKkTwKjE3ycaqaNUkFLHqXuq4WAvAmNigNEefCGNai/pKeBkoJQkKkmpIKWOU/e0SMkRY8XfY2aDitIXAUPNbGlLjGsJHnjUSRNXcjhODmjRJEexij6R3q8l9TpOa8EDj6aAC3mdAj5EdJwUqaXYd6ykLZt4/AhJ/5iOdY5TfWrZg40FSjqYpHLrgI0A3MGcuqFWYt+jCJrDG+IrKx3jMT+V9CghIM4ESS/Eem+OjwTGA6fGY/Zsie2OkwXVmOTYDjjezB6TdDVFYl9J3yWIfQ8pHGBmt0k6CZhYCGoTA958ZGbD4/7bwDZm9rGk7ma2XNIUYFVRzI/PiSr8cQBtNulVhVNznJaRJ7EvwB8S2/MJPdyxwNpKDnaxr5M3quFgLRb7JkiKeQ8GLgd2AWYrLDnrOHVFLcW+K1k/TuLnxLeV+5rZDOB0oDvQpaFjHCeP1FLsew1h0Ye5MYpvkjbA9TGm/RzCi5nLgT8BR/gkh1MvuNjXcZqBi30dJwd44NEq4hrELw/egzlODsi9g0maGVe5dJy6I/cOVo4G9IqOkxsyeXgr6SeEBfjeBJYSlpg9hLB22D6E51zHm9lf4pT974GBhEcAHRP1rAIuBv4Z+CFfPHNznFySuoPF4d2RwE6xvWcJDgbQNuoVRwJnEkK3/T9gtZkNljQ4li/QGXjOzH5api3XIjq5Iosh4nBCDPo1ZraS8LC4wB3xb2HRdIC9CJpGzGw+QZNYYB1we7mGXIvo5I0sHEwN5BUWTl/H+r1puWcHH/nysU49kYWDPQocKqmDpC4EEW9DPEJcMF3SIGBwyvY5Tmqkfg9mZs9IuhuYR1gMfRbQ0MoMVwC/j9rGucDTadvoOGmRiZJDUhczWyWpE6GHGmdmzzZ2XEtwLaKTJpUqObJ6x2pqXMi8AzAtbedynLyQiYOZ2XeyaMdx8karfUu4FoFHXezrFFO3UinHqQdq4mBFAUhLinljkNF7srfOcaqH92COkyJVcbDmBiBNcHTMf7lUrI3Y410n6WFJr0g6oRp2O07aVLMH2w6YamaDgQ8oCkAKXEYIQFqKtrHMKQTRbykGE1QgewA/LRXXXtI4SbMkzVq3uqFn2Y6TDdV0sJYEIC0l+i2mIBheCswAhhUXcLGvkzeq6WAtCUBaTvRbSf2Ok1uq6WDNDUBaKYdFwfBmhFVWnmlBXY6TCdV0sOYGIK2Up4F7gSeByWb2dkuMdZwsqIrYN+0ApJIm0cCqKqVwsa+TJh62zXFygAcebSauO/xy4z2Y4+SAVB1MUndJ32+kzJAYVaqxunwBdKfuSLsH684Xio5yDAEadTB8AXSnDknbwc4D+sf1vG5N9lSSrpE0CjgLGBXLjJLUQ9JdUdP4pKTBvgC6U6+k/cLlGcAgMxsi6QjCw+b7JG0M7EsIMtqRMJV/EoCkS4E5Zna4pG8C18bjG1wAPR7rgUedXJHlJMefgW9Kag8cBDxiZmtKlBsOXAdgZg8Dm0mqSFjoWkQnb2TmYGb2ETCTEFd+FHBzmaKlApW2zmcJTqsnbQcrXrT8ZuB7wJ7AA2XKJAOPjgCWmtkHJco5Tu5J1cHMbBnwmKTnJF0ITCfEnv8vM/skFpsBDCxMcgCTgKFR03geMCaW8wXQnbqj1So5XIvopIkrORwnB7iDOU6KeODRJuIiX6cpeA/mOClSNQfLIlCopMPjIhKOUxfUWw92OGFxdMepCxq9B5PUGbgF+CrQBpgMvA78irAo+ccEXWHymEnANkBv4OvAacDuBInUW8ChZvappF2Ai4EuwFJgrJktkdQfuBzoBawGTiDE+fgWsLek/wSONLPXWnLyjpM2lUxyHAi8bWYHA0Rd4BxgVFy9chOglKawP7APocd5guAQp0u6EzhY0r3ApcBhZva3+JD5bOA4YCow3sxekbQb8Bsz+2ZcKfMeM7utlKEu9nXyRiUOtgC4SNL5wD3AcmCJmT0DEGVMSBtICP8ce6kFhJ7v/kR9/QiRgAcBD8Zj2wBL4jrO/wjcmqizfSUnY2ZTCc5J+94DWucTdKeuaNTBzOzlOJQbCZxLkDtV8uX9OB7/maRP7QvJyGexXQHPm9l60X5jj7jczIZUfhqOk08aneSIMeBXm9n1wEWEe6ktJe0a87tKas7ztIVAr0KwUkntJO0Qe8Q3JB0d0yXpH+IxLvh16opKHGNH4EJJnwGfEl6SFHCppI6E+6/9mtqwmX0i6Sjg1/G+ri1hcYjnCWr6K+JkRjuCCn9e/HulpAnAUT7J4eQdF/s6TjNwsa/j5ADXIlaIaxCd5uA9mOOkSOYO1hLNoqRTJHWqtk2Okxb11oOdAriDOXVD1e7BmqlZHEaYmi9M93/PzBZKagOcT4hAZcCVhEcDWwIzJC01s32qZbvjpEU1Jzmao1l8CdjLzNZK2g84BziSoCfcBtgp5vUws/clnQbsU269MdciOnmjmg7WHM1iN2CapAGEnqpdTN8PmGJma+Ox71digGsRnbxRtXswM3sZ2IXgaOcCR9C4ZnEyMCOujHko0CGmq4JjHSf3VPON5uZoFrsR3g8DGJtInw6ML5SX1COmuxbRqSuqOURsjmbxAsIQ8TTg4UT6VYQXNedL+pQwyXEZYfj3Z0lLfJLDqQdci+g4zcC1iI6TA9zBHCdFXOxbAS70dZqL92COkyK5cjBJ6+LyRIXPGTH9EElzJM2T9IKkE2ttq+NUQt6GiGuKg91IakeYnh9mZn+NS9D2q4VxjtNU8uZgpehKsHMZgJl9TAiY4zi5J1dDRKBj0RBxVNQh3g0slnSTpNGSStotaZykWZJmrVu9IlvLHacEeevBNhgiApjZv0rakaAEmQjsz/rSqkI5F/s6uSJvPVhZzGyBmf2S4FxH1toex6mE3DuYpC6SRiSShgCLa2SO4zSJvA0RO0qam9i/n7AgxOmSfksQDH9IieGh4+SRXDmYmbUpkzWyqXXt2Kcbs1yB4dSY3A8RHaeeyVUPVk2aq0V03aFTTbwHc5wUyV0PJunHwHeAdYS1xE4khHDrzRdRqV41s6NqY6HjVE6uHCyuFXYIsLOZfSypJ7BxzB5tZv6KslNX5MrBCL3U0qg3pBD/sMTytI5TF+TtHmw60FfSy5J+I2nvRN4NCY3ihaUOdi2ikzdy1YOZ2aq4HvSewD7AHwrvhFHBENG1iE7eyJWDAZjZOmAmMFPSAmBMbS1ynOaTqyGipO1iGO0Crjt06pq89WBdCIFKuwNrgVcJizncRrgHK0zTLzWzJi+87jhZ44FHHacZeOBRx8kB7mCOkyJ5uwerGk0V+7rI10kD78EcJ0Vq7mCSTNIvEvsTJU1K7I+T9FL8PC1peE0MdZxmUHMHIyyO/n+isHc9JB1CUNMPN7PtgfHAjZK2yNhGx2kWeXCwtQR506kl8v4d+FFB9GtmzwLTgB9kZ57jNJ88OBjA5cBoSd2K0ncAZhelzYrpG+BiXydv5MLBzOwD4FpgQgXFyy6QbmZTzWyomQ1t06nYVx0ne3LhYJFLgOOBzom0F4BdisrtHNMdJ/fkxsFiDPpbCE5W4ALgfEmbAUgaQoiJ+JvMDXScZpC3B82/AE4q7JjZ3ZL6AI9LMmAlcKyZLamVgY7TFFzs6zjNwMW+jpMD8jZErBqltIiuN3Syxnswx0kRdzDHSZG6dTBJ5VZicZzckImDSZos6eTE/tmSJkj6kaRnJM2X9LNE/l2SZkt6XtK4RPoqSWdJegrYIwvbHaclZNWD/Y4Yfi0uYH4M8C4wABhGiB61i6S9YvnjzGwXYCgwofCgmaDyeM7MdjOzR4sbcS2ikzcymUU0s0WSlknaCdgcmAPsChwQtyFElBoAPEJwqiNiet+YvoywIMTtDbTjgUedXJHlNP1VBJnTFsDVwL7AuWb222ShuB7zfsAeZrZa0kygQ8z+KAYmdZy6IMtJjjuBAwk91wPxc5ykLgCS+kj6CtAN+Ht0ru2B3TO00XGqSmY9mJl9ImkGsDz2QtMlfQN4Iq6esgo4lrDw+XhJ84GFwJNZ2eg41SYzLWKc3HgWONrMXkm7PdciOmmSKy2ipIGEMNgPZeFcjpMXsppFfAHYNou2HCdPfGnEvi70dWpB3UqlHKceyE0PFmMdXkKYxv8YWESYyv9eolhbQkSpgWb2YtY2Ok5TyYWDKczT3wlMM7NjYtoQoKuZ/SpR7hxgrjuXUy/kwsEI6zF/amZTCglmNjdZIOoU/y8hqpTj1AV5uQcbxIYBRj8nrnj5e2BMjKFYrpyLfZ1ckRcHa4wrgOvN7LGGCnngUSdv5MXBnmfDAKMASBoD9AMmZ2mQ41SDvDjYw0B7SScUEiTtKmlv4GxgtJmtrZl1jtNMcjHJYWYW3/+6RNIZwEeEafoOhJcs74iC4AL/ZmZ/ydxQx2kiHnjUcZpBrsS+jvNlpdU6WEGL2JSF0B2n2rRaB3OcPJAbB5O0TtLcGKptnqTT4kuaSBohaUXML3z2q7XNjtMYuZhFjKwxsyEAMTbHjYT4HGfG/L+Y2SG1Ms5xmkNuerAkZvYeMA44SUXz845TT+SpB1sPM3s9DhG/EpP2lJQUAB9pZq8lj4lRgMcBtNmkVzaGOk4D5NbBIsneq9EhogcedfJGLoeIAJK2JUTyfa/WtjhOc8mlg0nqBUwBLrPWKjVxvhTkaYjYMd5jtQPWAtcBFyfyi+/Bfm5mt2VpoOM0ldw4mJmVXe/LzGYSpuwrZsc+3ZjlkaScGpPLIaLjtBbcwRwnRVqtgy14y2NyOLWn1TqY4+SB3DiYpC0k3SzpNUkvSLpP0tclPVdUbpKkibWy03GaQi5mERsIPLp5TQ1znBaSlx6sXODRN2tnkuO0nFz0YDQceLR/0QPmLYCLShV0sa+TN/LiYA3xWuE9MQj3YOUKutjXyRt5GSKWDTzqOPVMXhysZOBRYOvameQ4LScXDhYV80cA+8dp+ueBScDbNTXMcVpIbu7BzOxtwvJExQwqKjepkvp27OOLPzi1Jxc9mOO0VtzBHCdF3MEcJ0XcwRwnRdzBHCdF3MEcJ0XcwRwnRdzBHCdF3MEcJ0Va7RKyklYCC2ttR4KewNJaG5HA7Wmchmza2swafScqN1KpFFhYyRq6WSFplttTnrzZA9WxyYeIjpMi7mCOkyKt2cGm1tqAItyehsmbPVAFm1rtJIfj5IHW3IM5Ts1xB3OcFGl1DibpQEkLJb0q6YwatN9X0gxJL0p6XtLJMX2SpLckzY2fkRnbtUjSgtj2rJjWQ9KDkl6JfzfNyJbtEtdhrqQPJJ2S5TWSdLWk95KRo8tdDwV+Hb9T8yXtXHFDZtZqPkAb4DVgW2BjYB4wMGMbegM7x+2uwMvAQEKMkYk1vDaLgJ5FaRcAZ8TtM4Dza/Q/e4cQ4CizawTsBewMPNfY9QBGAn8mrBm+O/BUpe20th5sGPCqmb1uZp8ANwOHZWmAmS0xs2fj9krgRaBPljY0gcOAaXF7GnB4DWzYlxD7cnGWjZrZI8D7RcnlrsdhwLUWeBLoLql3Je20Ngfrw/rhtv9KDb/ckvoBOwFPxaST4hDj6qyGYwkMmC5pdoyADLC5mS2B8MMAfCVjmwCOAW5K7NfyGpW7Hs3+XrU2B1OJtJo8h5DUBbgdOMXMPgCuAPoDQ4AlwC8yNumfzGxn4CDgB5L2yrj9DZC0MfAt4NaYVOtrVI5mf69am4P9Feib2P8qNYitKKkdwbluMLM7AMzsXTNbZ2afAVcShrOZYSEsHmb2HmElm2HAu4WhTvz7XpY2EZz9WTN7N9pW02tE+evR7O9Va3OwZ4ABkraJv47HAHdnaUBciul3wItmdnEiPTlmPwJ4rvjYFG3qLKlrYRs4ILZ/NzAmFhsD/DErmyLfJjE8rOU1ipS7HncD342zibsDKwpDyUbJetYog9mhkYSZu9eAH9eg/eGE4cN8YG78jASuAxbE9LuB3hnatC1hRnUeYR2AH8f0zYCHgFfi3x4Z2tQJWAZ0S6Rldo0Ijr0E+JTQQx1f7noQhoiXx+/UAmBope24VMpxUqS1DREdJ1e4gzlOiriDOU6KuIM5Toq4gzlOiriDtRBJ66Ly+zlJf5LUvYJjVjWS313S9xP7W0q6rQq29kuqx7NA0pCs3xzIE+5gLWeNmQ0xs0EE8egPqlBnd+BzBzOzt83sqCrUmymS2hJkT+5gTlV4goQIVNKPJD0Txas/Ky4sqYukhyQ9G9/VKij/zwP6x57xwmTPI+kpSTsk6pgpaZeo1rg6tjcnUVdJJI2VdFfsdd+QdJKk0+KxT0rqkaj/EkmPx156WEzvEY+fH8sPjumTJE2VNB24FjgLGBXPZZSkYbGuOfHvdgl77pB0f3wf64KErQfGazRP0kMxrUnnWzOyVjq0tg+wKv5tQxCtHhj3DyAETRHhh+weYK+iY9oCm8TtnsCrsXw/1n9P6fN94FTgZ3G7N/By3D4HODZudyeoWToX2ZqsZ2xsryvQC1gBjI95vySIlAFmAlfG7b0Sx18KnBm3vwnMjduTgNlAx0Q7lyVs2ARoG7f3A25PlHsd6AZ0ABYT9H+9CEr2bWK5HpWebx4+rTnwaFZ0lDSX8OWdDTwY0w+InzlxvwswAHgkcayAc6Ky/TNC77d5I+3dEts4k7CmdUGJfgDwLUkT434HYCvC+2jlmGHhnbWVklYAf4rpC4DBiXI3QXiHStIm8T5zOHBkTH9Y0maSCgtj321ma8q02Q2YJmkAQVLWLpH3kJmtAJD0AuElzE2BR8zsjdhW4R2u5pxv5riDtZw1ZjYkfrnuIdyD/ZrgPOea2W8bOHY04Rd6FzP7VNIiwhelLGb2lqRlcUg2CjgxZgk40syaEi7848T2Z4n9z1j/u1GspzMafoXjwwbanExw7CPi+3Izy9izLtqgEu1D8843c/werErEX94JwMT4usoDwHHxvTAk9ZFU/EJjN+C96Fz7EH6xAVYShm7luBk4nSCUXRDTHgD+Lar5kbRTNc4rMirWOZygJF9B6IlHx/QRwFIL770VU3wu3YC34vbYCtp+Athb0jaxrR4xPc3zrRruYFXEzOYQFOvHmNl04EbgCUkLgNvY0GluAIYqBKEZDbwU61kGPBYnFS4s0dRthFdxbkmkTSYMt+bHCZHJ1Tsz/i7pcWAKQXUO4V5rqKT5hEmZMWWOnQEMLExyEOJenCvpMcJ9a4OY2d+AccAdkuYBf4hZaZ5v1XA1vdMgkmYSAtHMqrUt9Yj3YI6TIt6DOU6KeA/mOCniDuY4KeIO5jgp4g7mOCniDuY4KfK/OS+5xSSE7vUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1161fd10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance = clf.feature_importances_\n",
    "\n",
    "# Make importances relative to max importance.\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.50458481,  17.16950591,  21.9655852 ,  28.16085081,\n",
       "        20.1262185 ,  57.5122793 ,  15.65724381,  13.97991887,\n",
       "        20.29594867, 100.        ,   1.20621248,   3.08694459,\n",
       "         2.23935062,  13.37131282,   8.24644623,  10.15523883])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that age and happiness are the most important features in predicting whether or not someone lives with a partner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRILL: Improve this gradient boost model\n",
    "\n",
    "While this model is already doing alright, we've seen from the Type I and Type II error rates that there is definitely room for improvement.  Your task is to see how low you can get the error rates to go in the test set, based on your model in the training set.  Strategies you might use include:\n",
    "\n",
    "* Creating new features\n",
    "* Applying more overfitting-prevention strategies like subsampling\n",
    "* More iterations\n",
    "* Trying a different loss function\n",
    "* Changing the structure of the weak learner: Allowing more leaves in the tree, or other modifications\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start of Challenge:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Approach #1: Up sample the minority class__\n",
    "\n",
    "Upsample after splitting data into train/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    5013\n",
      "2.0    3134\n",
      "Name: partner, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect class imbalance in full dataset\n",
    "print((df['partner']).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab X and Y from above to reset data processing\n",
    "# Set our outcome to 0 and 1.\n",
    "y = df['partner'] - 1\n",
    "X = df.loc[:, ~df.columns.isin(['partner', 'cntry', 'idno'])]\n",
    "\n",
    "# Make the categorical variable 'country' into dummies.\n",
    "X = pd.concat([X, pd.get_dummies(df['cntry'])], axis=1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    4520\n",
      "1.0    2812\n",
      "Name: partner, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Recombine X and y training data sets so we can upsample\n",
    "# Convert y_train back into a data frame\n",
    "y_train_df = y_train.to_frame()\n",
    "\n",
    "# Join with X_train to create a full training data set with predictors and outcome variable\n",
    "training = y_train_df.join(X_train)\n",
    "\n",
    "# Look at class imbalance in training dataset\n",
    "print((training['partner']).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    4520\n",
       "0.0    4520\n",
       "Name: partner, dtype: int64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate training data into majority and minority classes\n",
    "training_minority = training[training.partner==1]\n",
    "training_majority = training[training.partner==0]\n",
    "\n",
    "# Import library\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Upsample minority class. Set n equal to training set majority class\n",
    "training_minority_upsampled = resample(training_minority,\n",
    "                                      replace = True, \n",
    "                                      n_samples = 4520,\n",
    "                                      random_state = 123)\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "training_upsampled = pd.concat([training_majority, training_minority_upsampled])\n",
    "\n",
    "# Display new class counts\n",
    "training_upsampled.partner.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>partner</th>\n",
       "      <th>year</th>\n",
       "      <th>tvtot</th>\n",
       "      <th>ppltrst</th>\n",
       "      <th>pplfair</th>\n",
       "      <th>pplhlp</th>\n",
       "      <th>happy</th>\n",
       "      <th>sclmeet</th>\n",
       "      <th>sclact</th>\n",
       "      <th>gndr</th>\n",
       "      <th>agea</th>\n",
       "      <th>CH</th>\n",
       "      <th>CZ</th>\n",
       "      <th>DE</th>\n",
       "      <th>ES</th>\n",
       "      <th>NO</th>\n",
       "      <th>SE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4860</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3257</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      partner  year  tvtot  ppltrst  pplfair  pplhlp  happy  sclmeet  sclact  \\\n",
       "4860      0.0     7    7.0      5.0      5.0     5.0    8.0      4.0     3.0   \n",
       "3257      1.0     6    4.0      8.0      8.0     8.0    9.0      7.0     3.0   \n",
       "82        1.0     6    2.0      4.0      5.0     5.0    9.0      7.0     3.0   \n",
       "1828      0.0     6    7.0      8.0      9.0     3.0    7.0      6.0     1.0   \n",
       "674       1.0     6    1.0      3.0      6.0     4.0    7.0      5.0     3.0   \n",
       "\n",
       "      gndr  agea  CH  CZ  DE  ES  NO  SE  \n",
       "4860   1.0  41.0   0   0   0   1   0   0  \n",
       "3257   2.0  18.0   0   0   0   1   0   0  \n",
       "82     2.0  35.0   1   0   0   0   0   0  \n",
       "1828   2.0  25.0   0   1   0   0   0   0  \n",
       "674    2.0  20.0   1   0   0   0   0   0  "
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break training set back into X_train and y_train\n",
    "X_train_upsampled = training.drop(['partner'], 1)\n",
    "y_train_upsampled = training['partner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04609929078014184\n",
      "Percent Type II errors: 0.17757774140752863\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.05276073619631902\n",
      "Percent Type II errors: 0.18773006134969325\n"
     ]
    }
   ],
   "source": [
    "# Rerun model with same settings as above\n",
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf2 = ensemble.GradientBoostingClassifier(**params)\n",
    "clf2.fit(X_train_upsampled, y_train_upsampled)\n",
    "\n",
    "predict_train = clf2.predict(X_train_upsampled)\n",
    "predict_test = clf2.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train_upsampled, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsampling the minority class did not improve either type error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Approach #2: Increase iterations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04487179487179487\n",
      "Percent Type II errors: 0.16953082378614293\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.05644171779141104\n",
      "Percent Type II errors: 0.18527607361963191\n"
     ]
    }
   ],
   "source": [
    "# Keep upsampled data\n",
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 1000,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf3 = ensemble.GradientBoostingClassifier(**params)\n",
    "clf3.fit(X_train_upsampled, y_train_upsampled)\n",
    "\n",
    "predict_train = clf3.predict(X_train_upsampled)\n",
    "predict_test = clf3.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train_upsampled, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04487179487179487\n",
      "Percent Type II errors: 0.16953082378614293\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.05644171779141104\n",
      "Percent Type II errors: 0.18527607361963191\n"
     ]
    }
   ],
   "source": [
    "# Get rid of upsampled data\n",
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 1000,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf4 = ensemble.GradientBoostingClassifier(**params)\n",
    "clf4.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf4.predict(X_train)\n",
    "predict_test = clf4.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doubling the number of iterations did not significantly improve the accuracy.  There was absolutely no difference between the test and upsampled test datasets.  Possible error here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Approach #3: Increase max depth__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.011320240043644299\n",
      "Percent Type II errors: 0.08333333333333333\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.08098159509202454\n",
      "Percent Type II errors: 0.17423312883435582\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "params = {'n_estimators': 1000,\n",
    "          'max_depth': 4,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf5 = ensemble.GradientBoostingClassifier(**params)\n",
    "clf5.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf5.predict(X_train)\n",
    "predict_test = clf5.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing max depth resulted in overfitting. \n",
    "\n",
    "Overall, was not successful in improving the model accuracy in any of the 3 approaches tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
